import os
import argparse
import pandas as pd
import numpy as np
import tensorflow as tf
from google.cloud import bigquery
from google.cloud import aiplatform
import time

# å®šç¾©ç‰¹å¾µçš„é…ç½®
NUMERICAL_FEATURES = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
CATEGORICAL_FEATURES = ['island']
# Label ä¸éœ€è¦æ”¾é€² Inputï¼Œä½†éœ€è¦åš One-Hot
LABEL_COLUMN = 'species'


def df_to_dataset(dataframe, shuffle=True, batch_size=32):
    """
    ä¿®æ­£ç‰ˆï¼šå°‡ Pandas DF è½‰æ›ç‚º tf.data.Dataset
    1. å¼·åˆ¶è½‰æ›æ•¸å€¼ç‚º float32
    2. å¼·åˆ¶å°‡ shape å¾ (N,) è½‰ç‚º (N, 1) ä»¥ç¬¦åˆ Keras Input(shape=(1,))
    """
    df = dataframe.copy()
    labels = df.pop(LABEL_COLUMN)

    # Label One-Hot (ä¿æŒä¸è®Š)
    labels = pd.get_dummies(labels, prefix=LABEL_COLUMN)

    # --- ğŸ”¥ é—œéµä¿®æ­£é–‹å§‹ ğŸ”¥ ---
    data_dict = {}

    # éæ­·æ‰€æœ‰ç‰¹å¾µæ¬„ä½ï¼Œæ‰‹å‹•èª¿æ•´å½¢ç‹€èˆ‡å‹åˆ¥
    for name, value in df.items():
        # å–å‡º numpy array
        val = value.values

        if name in NUMERICAL_FEATURES:
            # æ•¸å€¼ç‰¹å¾µï¼šè½‰ float32 ä¸¦å¢åŠ ä¸€å€‹ç¶­åº¦
            # ä¾‹å¦‚: [0.1, 0.5] -> [[0.1], [0.5]]
            val = val.astype('float32')[:, np.newaxis]
        else:
            # å­—ä¸²ç‰¹å¾µï¼šé›–ç„¶ä¸ç”¨è½‰ floatï¼Œä½†ä¹Ÿè¦å¢åŠ ç¶­åº¦
            val = val[:, np.newaxis]

        data_dict[name] = val
    # --- ğŸ”¥ é—œéµä¿®æ­£çµæŸ ğŸ”¥ ---

    # é€™è£¡å‚³å…¥è™•ç†å¥½çš„ data_dict
    ds = tf.data.Dataset.from_tensor_slices((data_dict, labels))

    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size)
    return ds


def train_model(project_id, model_dir, bucket_name):
    # --- 1. å¾ BigQuery è®€å–è³‡æ–™ ---
    print("Loading data from BigQuery...")
    client = bigquery.Client(project=project_id)
    query = """
        SELECT species, island, culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g
        FROM `bigquery-public-data.ml_datasets.penguins`
        WHERE body_mass_g IS NOT NULL
    """
    df = client.query(query).to_dataframe()
    df.dropna(inplace=True)

    # åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰é›†
    train_df = df.sample(frac=0.8, random_state=0)
    test_df = df.drop(train_df.index)

    # è½‰ç‚º tf.data.Dataset
    batch_size = 32
    train_ds = df_to_dataset(train_df, batch_size=batch_size)
    test_ds = df_to_dataset(test_df, shuffle=False, batch_size=batch_size)

    # --- 2. å»ºç«‹æ¨¡å‹ (åŒ…å«å‰è™•ç†) ---
    all_inputs = {}
    encoded_features = []

    # A. è™•ç†æ•¸å€¼ç‰¹å¾µ
    for header in NUMERICAL_FEATURES:
        numeric_col = tf.keras.Input(shape=(1,), name=header, dtype="float32")
        normalization_layer = tf.keras.layers.Normalization()

        # ğŸ”¥ ä¿®æ”¹ï¼šç¢ºä¿ adapt ç”¨çš„è³‡æ–™ä¹Ÿæ˜¯ (N, 1) ä¸” float32
        adapt_data = train_df[header].values.astype('float32')[:, np.newaxis]
        normalization_layer.adapt(adapt_data)

        encoded_numeric_col = normalization_layer(numeric_col)
        all_inputs[header] = numeric_col
        encoded_features.append(encoded_numeric_col)

    # B. è™•ç†é¡åˆ¥ç‰¹å¾µ
    for header in CATEGORICAL_FEATURES:
        cat_col = tf.keras.Input(shape=(1,), name=header, dtype="string")
        lookup_layer = tf.keras.layers.StringLookup(output_mode="one_hot")

        # ğŸ”¥ ä¿®æ”¹ï¼šç¢ºä¿ adapt ç”¨çš„è³‡æ–™ä¹Ÿæ˜¯ (N, 1)
        adapt_data = train_df[header].values[:, np.newaxis]
        lookup_layer.adapt(adapt_data)

        encoded_cat_col = lookup_layer(cat_col)
        all_inputs[header] = cat_col
        encoded_features.append(encoded_cat_col)

    # --- çµ„åˆæ¨¡å‹ (Functional API) ---
    all_features = tf.keras.layers.concatenate(encoded_features)

    x = tf.keras.layers.Dense(32, activation="relu")(all_features)
    x = tf.keras.layers.Dense(32, activation="relu")(x)
    output = tf.keras.layers.Dense(3, activation="softmax")(x)

    # å»ºç«‹æ¨¡å‹ï¼ŒæŒ‡å®š Inputs (å­—å…¸) å’Œ Outputs
    model = tf.keras.Model(inputs=all_inputs, outputs=output)

    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # --- 3. è¨“ç·´ ---
    model.fit(train_ds, epochs=10, validation_data=test_ds)

    # --- 4. è©•ä¼°èˆ‡ç´€éŒ„ ---
    loss, accuracy = model.evaluate(test_ds)
    print(f"Test Accuracy: {accuracy}")

    # Vertex AI Logging (çœç•¥éƒ¨åˆ†é‡è¤‡ä»£ç¢¼...)
    timestamp = int(time.time())
    run_id = f"penguin-run-{timestamp}"
    aiplatform.init(project=project_id, experiment='penguin-experiment', location='asia-east1',
                    staging_bucket=f'gs://{bucket_name.replace("gs://", "")}')
    aiplatform.start_run(run=run_id)
    aiplatform.log_metrics({"accuracy": accuracy, "loss": loss})
    aiplatform.end_run()

    # --- 5. å„²å­˜æ¨¡å‹ ---
    print(f"Saving model to {model_dir}")
    # export æœƒä¿å­˜åŒ…å« StringLookup å’Œ Normalization çš„å®Œæ•´æ¨¡å‹
    try:
        model.export(model_dir)
    except AttributeError:
        # å¦‚æœç’°å¢ƒä¸å°å¿ƒé€€å› Keras 2 (TF < 2.16)ï¼Œexport ä¸å­˜åœ¨ï¼Œæ”¹ç”¨ save
        tf.saved_model.save(model, model_dir)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--project_id', type=str, required=True)
    parser.add_argument('--model_dir', type=str, default=os.environ.get('AIP_MODEL_DIR'))
    parser.add_argument('--bucket_name', type=str)
    args = parser.parse_args()

    train_model(args.project_id, args.model_dir, args.bucket_name)